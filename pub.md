
# 📝 Publications 
My full paper list can be found at <a href='https://scholar.google.com/citations?user=i5YUH14AAAAJ&hl=zh-CN'><img src="https://img.shields.io/badge/Google%20Scholar-orange"></a>
                                                                                                   
<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/MetaEarth.png"><img src='images/MetaEarth.png' alt="MetaEarth" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="https://arxiv.org/abs/2405.13570"><b>MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation</b></a> \\
 *Under review, 2024* \\
Zhiping Yu, **<font color="#C00000">Chenyang Liu</font>**, Liqin Liu, Zhenwei Shi, Zhengxia Zou \\
[<a href="https://arxiv.org/abs/2405.13570">PDF</a>] \\
Media report: [<a href="https://mp.weixin.qq.com/s/IEe-tj4B0QWk6hX_fI-UHA">把整个地球装进神经网络，北航团队推出全球遥感图像生成模型</a>] -- <a href="https://www.nsfc.gov.cn/publish/portal0/tab448/info92986.htm">国家自然科学基金委</a> | <a href="https://mp.weixin.qq.com/s/IEe-tj4B0QWk6hX_fI-UHA">量子位</a> | <a href="https://mp.weixin.qq.com/s/JrFUIR2rLxJFMst7JXL3Mw">中国图象图形学学会CSIG</a> | <a href="https://mp.weixin.qq.com/s/Dn51xNhfM6e5JKO2XWq6ag">CVer</a> | <a href="https://mp.weixin.qq.com/s/K-SrPA5vRUiuXUN3ZxOALA">惠天地</a> | <a href="https://mp.weixin.qq.com/s/XnkyDefw7SpZmySdaIczag">深圳市人工智能产业协会</a>

</div>
</div> -->


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/RSCaMa.png"><img src='images/RSCaMa.png' alt="RSCaMa" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="https://ieeexplore.ieee.org/document/10537177"><b>RSCaMa: Remote Sensing Image Change Captioning with State Space Model</b></a> \\
 *IEEE Geoscience and Remote Sensing Letters (GRSL), 2024* \\
**<font color="#C00000">Chenyang Liu</font>**, Keyan Chen, Bowen Chen, Haotian Zhang, Zhengxia Zou, Zhenwei Shi \\
[<a href="https://ieeexplore.ieee.org/document/10537177">PDF</a>] [<a href="https://github.com/Chen-Yang-Liu/RSCaMa">Code</a>] \\
Media report: [<a href="https://mp.weixin.qq.com/s/EznAxqSw3eBh68IbP-_P9w">RSCaMa：将Mamba用于遥感变化描述任务中，充分利用状态空间模型的特征选择性建模能力！</a>] -- <a href="https://mp.weixin.qq.com/s/EznAxqSw3eBh68IbP-_P9w">GISer阿兴</a> | <a href="https://mp.weixin.qq.com/s/FENxlWZqWERS7fFWLwWwTA">CV看交通</a>

</div>
</div> -->

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/Change_Agent.png"><img src='images/Change_Agent.png' alt="Change_Agent" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://ieeexplore.ieee.org/document/10591792"><b>Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis</b></a> \\
 *IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2024* \\
**<font color="#C00000">Chenyang Liu</font>**, Keyan Chen, Haotian Zhang, Zipeng Qi, Zhengxia Zou, Zhenwei Shi \\
[<a href="https://ieeexplore.ieee.org/document/10591792">PDF</a>] [<a href="https://github.com/Chen-Yang-Liu/Change-Agent">Code</a>] \\
Media report: [<a href="https://mp.weixin.qq.com/s/FENxlWZqWERS7fFWLwWwTA">Change-Agent：可交互的遥感变化解译智能体</a>] -- <a href="https://mp.weixin.qq.com/s/FENxlWZqWERS7fFWLwWwTA">遥感与深度学习</a> | <a href="https://mp.weixin.qq.com/s/-iaJjsaEsKr7lkMDEmANVw">我爱计算机视觉</a> | <a href="https://mp.weixin.qq.com/s/aQZTt02obm3MfOtLBAmPTQ"> AIWalker</a> | <a href="https://www.douyin.com/video/7353211020686396713">抖音</a>

</div>
</div> -->

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/Attack.gif"><img src='images/Attack.gif' alt="Attack" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="https://ieeexplore.ieee.org/document/10521633"><b>Digital-to-Physical Visual Consistency Optimization for Adversarial Patch Generation in Remote Sensing Scenes</b></a> \\
 *IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2024* \\
Jianqi Chen, Yilan Zhang, **<font color="#C00000">Chenyang Liu</font>**, Keyan Chen, Zhengxia Zou, Zhenwei Shi  \\
[<a href="https://ieeexplore.ieee.org/document/10521633">PDF</a>] [<a href="https://github.com/WindVChen/VCO-AP">Code</a>]

</div>
</div> -->


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/RSPrompter.png"><img src='images/RSPrompter.png' alt="RSPrompter" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="https://arxiv.org/abs/2306.16269"><b>RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</b></a> \\
 *IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2024* \\
Keyan Chen, **<font color="#C00000">Chenyang Liu</font>**, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi  \\
[<a href="https://arxiv.org/abs/2306.16269">PDF</a>] [<a href="https://kyanchen.github.io/RSPrompter/">Page</a>] [<a href="https://github.com/KyanChen/RSPrompter">Code</a>] [<a href="https://huggingface.co/spaces/KyanChen/RSPrompter">Demo</a>]

</div>
</div> -->

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/Prompt_CC.png"><img src='images/Prompt_CC.png' alt="Prompt_CC" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="https://ieeexplore.ieee.org/document/10271701"><b>A Decoupling Paradigm with Prompt Learning for Remote Sensing Image Change Captioning</b></a> \\
 *IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2023* \\
**<font color="#C00000">Chenyang Liu</font>**, Rui Zhao, Jianqi Chen, Zipeng Qi, Zhengxia Zou, and Zhenwei Shi  \\
[<a href="https://ieeexplore.ieee.org/document/10271701">PDF</a>] [<a href="https://github.com/Chen-Yang-Liu/PromptCC">Page</a>]

</div>
</div> -->

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/Seg_INR.png"><img src='images/Seg_INR.png' alt="Seg_INR" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="https://ieeexplore.ieee.org/abstract/document/10149540"><b>Implicit Ray-Transformers for Multi-view Remote Sensing Image Segmentation</b></a> \\
 *IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2023* \\
Zipeng Qi, Hao Chen, **<font color="#C00000">Chenyang Liu</font>**, Zhenwei Shi, and Zhengxia Zou \\
[<a href="https://ieeexplore.ieee.org/abstract/document/10149540">PDF</a>] [<a href="https://qizipeng.github.io/IRT/">Page</a>]

</div>
</div> -->




<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/advei.gif"><img src='images/advei.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://arxiv.org/abs/2403.14163"><b>Enhancing Multimodal-Input Object Goal Navigation by Leveraging Large Language Models for Inferring Room-Object Relationship Knowledge
</b></a> \\
*Under review* \\
**<font color="#000000">Leyuan Sun*</font>**, [Asako Kanezaki](https://kanezaki.github.io/index_jp.html), [Guillaume Caron](https://home.mis.u-picardie.fr/~g-caron/fr/), and Yusuke Yoshiyasu\\
[<a href="https://arxiv.org/abs/2403.14163">PDF</a>] [<a href="https://sunleyuan.github.io/ObjectNav/">**<font color="#C00000">Project page</font>**</a>]

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/ECCV.gif"><img src='images/ECCV.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://eccv2024.ecva.net/"><b>DiffSurf: A Transformer-based Diffusion Model for Generating and Reconstructing 3D Surfaces in Pose
</b></a> \\
*The 18th European Conference on Computer Vision ECCV 2024, Milano, Italy, (CCF-B)* \\
Yusuke Yoshiyasu* and **<font color="#000000">Leyuan Sun</font>** \\
[<a href="https://eccv2024.ecva.net/">PDF</a>] [<a href="https://github.com/yusukey03012">Code</a>] 

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/ana.gif"><img src='images/ana.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://link.springer.com/article/10.1007/s12369-023-01096-9"><b>A Cybernetic Avatar System to Embody Human Telepresence for Connectivity, Exploration, and Skill Transfer</b></a> \\
*International Journal of Social Robotics, 2024 (SCI收录，中科院二区，JCR Q1)* \\
Rafael Cisneros-Limón*, Antonin Dallard, Mehdi Benallegue, Kenji Kaneko, Hiroshi Kaminaga, Pierre Gergondet, Arnaud Tanguy, Rohan Pratap Singh, **<font color="#000000">Leyuan Sun</font>**, Yang Chen, Carole Fournier, Guillaume Lorthioir, Masato Tsuru, Sélim Chefchaouni-Moussaoui, Yukiko Osawa, Guillaume Caron, Kevin Chappellet, Mitsuharu Morisawa, Adrien Escande, Ko Ayusawa, Younes Houhou, Iori Kumagai, Michio Ono, Koji Shirasaka, Shiryu Wada, Hiroshi Wada, Fumio Kanehiro and Abderrahmane Kheddar\\
[<a href="papers/IJSR.pdf">PDF</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/janus/team-janus.html">**<font color="#C00000">ANA Avatar XPRIZE team web</font>**</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/project-janus.html">**<font color="#C00000">Project page</font>**</a>] 

</div>
</div>






<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/sensors.gif"><img src='images/sensors.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://ieeexplore.ieee.org/abstract/document/10214516"><b>TransFusionOdom: Transformer-Based LiDAR-Inertial Fusion Odometry Estimation</b></a> \\
*IEEE Sensors Journal, vol. 23, no. 18, pp. 22064-22079, 2023, (SCI收录，中科院二区TOP，JCR Q1)* \\
**<font color="#000000">Leyuan Sun*</font>**, [Guanqun Ding](https://github.com/gqding), [Yue Qiu](https://qiuyue1993.github.io/qiuyue.github.com/), Y. Yoshiyasu and F. Kanehiro \\
[<a href="papers/sensors.pdf">PDF</a>] [<a href="https://github.com/RakugenSon/Multi-modal-dataset-for-odometry-estimation">Dataset</a>] 

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/ICRA.gif"><img src='images/ICRA.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://www.ais.uni-bonn.de/ICRA2023AvatarWS/contributions/ICRA_2023_Avatar_WS_Cisneros.pdf"><b>Enhancement of Team JANUS’ cybernetic avatar system for exploration and skill transfer</b></a> \\
*ICRA 2023 Workshop on "2nd Workshop Toward Robot Avatars", London, (CCF-B)* \\
R. Cisneros*, A. Dallard, M. Benallegue, K. Kaneko, H. Kaminaga, P. Gergondet, A. Tanguy, C. Fournier, R. Singh, Y. Chen, S. Chefchaouni-Moussaoui, G. Lorthioir, Y. Osawa, M. Tsuru, **<font color="#000000">Leyuan Sun</font>**, M. Morisawa, G. Caron, M. Ono, K. Shirasaka, S. Wada, H. Wada, F. Kanehiro and A. Kheddar\\
[<a href="papers/ICRA.pdf">PDF</a>] [<a href="https://youtube.com/watch?v=CaOOoSqWjCo">Youtube full demo</a>] [<a href="https://www.ais.uni-bonn.de/ICRA2023AvatarWS/">Workshop page</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/janus/team-janus.html">**<font color="#C00000">ANA Avatar XPRIZE team web</font>**</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/project-janus.html">**<font color="#C00000">Project page</font>**</a>] 

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/robio.gif"><img src='images/robio.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://ieeexplore.ieee.org/document/10011808"><b>CertainOdom: Uncertainty Weighted Multi-task Learning Model for LiDAR Odometry Estimation</b></a> \\
 *2022 IEEE International Conference on Robotics and Biomimetics (ROBIO), Jinghong, China, (EI收录)* \\
**<font color="#000000">Leyuan Sun*</font>**, [Guanqun Ding](https://github.com/gqding), Y. Yoshiyasu and F. Kanehiro \\
[<a href="papers/RSS.pdf">PDF</a>] [<a href="https://www.sie.tsukuba.ac.jp/news_award/2022/12/12/9032">**<font color="#C00000">Best Conference Paper Award</font>**</a>] 

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/RSS.gif"><img src='images/RSS.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://unit.aist.go.jp/jrl-22022/en/projects/janus/papers/Cisneros_RSS2022-Workshop.pdf"><b>Avatar system of Team JANUS: A cybernetic avatar to transport human presence to remote locations</b></a> \\
 *“Toward Robot Avatars: Perspectives on the ANA Avatar XPRIZE Competition", New York City, USA, RSS 2022 Workshop* \\
R. Cisneros*, M. Benallegue, K. Kaneko, H. Kaminaga, G. Caron, A. Tanguy, R. Singh,  **<font color="#000000">Leyuan Sun</font>**, A. Dallard, C. Fournier, M. Tsuru, C. Yang, Y. Osawa, G. Lorthioir, F. Kanehiro and A. Kheddar \\
[<a href="papers/RSS.pdf">PDF</a>] [<a href="https://www.youtube.com/watch?v=GnGmWgzANWU">Youtube full demo</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/janus/team-janus.html">**<font color="#C00000">ANA Avatar XPRIZE team web</font>**</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/project-janus.html">**<font color="#C00000">Project page</font>**</a>] 


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/cslam.gif"><img src='images/cslam.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://ieeexplore.ieee.org/document/9999740"><b>Enhanced Visual Feedback with Decoupled Viewpoint Control in Immersive Humanoid Robot Teleoperation using SLAM</b></a> \\
 *2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids), Ginowan, Japan, (EI收录)* \\
Yang Chen*, **<font color="#000000">Leyuan Sun* (equal contribution)</font>**, Mehdi Benallegue, Rafael Cisneros Limon, Rohan Pratap Singh, Kenji Kaneko, Arnaud TANGUY, Guillaume Caron, Kenji Suzuki, Abderrahmane Kheddar and Fumio Kanehiro \\
[<a href="papers/humanoids.pdf">PDF</a>] [<a href="https://www.youtube.com/watch?v=Jdiaosp_qH8">Youtube full demo</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/janus/team-janus.html">**<font color="#C00000">ANA Avatar XPRIZE team web</font>**</a>] [<a href="https://unit.aist.go.jp/jrl-22022/en/projects/project-janus.html">**<font color="#C00000">Project page</font>**</a>] 


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/semantic_mapping.gif"><img src='images/semantic_mapping.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://www.fujipress.jp/jrm/rb/robot003300061385/"><b>Visual SLAM Framework Based on Segmentation with the Improvement of Loop Closure Detection in Dynamic Environments</b></a> \\
 *Journal of Robotics and Mechatronics, vol.33 no.6, 2021, (EI期刊)* \\
**<font color="#000000">Leyuan Sun*</font>**, Rohan P. Singh, and Fumio Kanehiro \\
[<a href="papers/JRM.pdf">PDF</a>]


</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/SII.png"><img src='images/SII.png' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://ieeexplore.ieee.org/document/9026299"><b>Multi-purpose SLAM framework for Dynamic Environment</b></a> \\
 *2020 IEEE/SICE International Symposium on System Integration (SII), Hawaii, USA, (EI收录)* \\
**<font color="#000000">Leyuan Sun*</font>**,  F. Kanehiro, I. Kumagai and Y. Yoshiyasu \\
[<a href="papers/SII.pdf">PDF</a>]


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/RSJ.gif"><img src='images/RSJ.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://www.rsj.or.jp/info/awards/category/ins/#:~:text=Leyuan%20Sun%20(University%20of%20Tsukuba%2C%20AIST)%0ARobust%20SLAM%20in%20Dynamic%20Environment%20based%20on%20Object%27s%20Mask%0A37th%20RSJ2019"><b>Robust SLAM in Dynamic Environment based on Object’s Mask</b></a> \\
 *the 37th Annual Conference of the Robotics Society of Japan (RSJ2019)* \\
**<font color="#000000">Leyuan Sun*</font>**, and Fumio Kanehiro \\
[<a href="papers/RSJ.pdf">PDF</a>] [<a href="https://www.rsj.or.jp/info/awards/category/ins/#:~:text=Leyuan%20Sun%20(University%20of%20Tsukuba%2C%20AIST)%0ARobust%20SLAM%20in%20Dynamic%20Environment%20based%20on%20Object%27s%20Mask%0A37th%20RSJ2019">**<font color="#C00000">International Session Best Presentation Award Finalist</font>**</a>]


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/robomech.gif"><img src='images/robomech.gif' alt="PSNet" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">


<a class=PaperTitle href="https://www.jstage.jst.go.jp/article/jsmermd/2019/0/2019_2A1-R07/_article/-char/en"><b>Robust SLAM based on Segmentation of Dynamic Object’s Point Cloud</b></a> \\
*JSME annual Conference on Robotics and Mechatronics (Robomec)*, 2019 \\
*DOI: [https://doi.org/10.1299/jsmermd.2019.2A1-R07](https://doi.org/10.1299/jsmermd.2019.2A1-R07)*\\
**<font color="#000000">Leyuan Sun*</font>**, and Fumio Kanehiro \\
[<a href="papers/robomech.pdf">PDF</a>]


</div>
</div>



<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/RSICCformer2.png"><img src='images/RSICCformer2.png' alt="RSICCformer" width="100%"> </a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="http://levir.buaa.edu.cn/publications/ChangeCaptioning.pdf"><b>Remote Sensing Image Change Captioning With Dual-Branch Transformers: A New Method and a Large Scale Dataset</b></a> \\
 *IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022* \\
**<font color="#C00000">Chenyang Liu</font>**, Rui Zhao, Hao Chen, Zhengxia Zou, and Zhenwei Shi \\
[<a href="https://levir.buaa.edu.cn/publications/ChangeCaptioning.pdf">PDF</a>] [<a href="https://github.com/Chen-Yang-Liu/RSICC">Dataset</a>] [<a href="https://github.com/Chen-Yang-Liu/RSICC">Code</a>] 

</div>
</div> -->

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/MLAT.png"><img src='images/MLAT.png' alt="MLAT" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<a class=PaperTitle href="http://levir.buaa.edu.cn/publications/Captioning-Based-on-Multilayer-Aggregated-Transformer.pdf"><b>Remote Sensing Image Captioning Based on Multi-Layer Aggregated Transformer</b></a> \\
 *IEEE Geoscience and Remote Sensing Letters (GRSL), 2022* \\
**<font color="#C00000">Chenyang Liu</font>**, Rui Zhao, and Zhenwei Shi \\
[<a href="http://levir.buaa.edu.cn/publications/Captioning-Based-on-Multilayer-Aggregated-Transformer.pdf">PDF</a>] [<a href="https://github.com/Chen-Yang-Liu/MLAT">Code</a>]

</div>
</div> -->
